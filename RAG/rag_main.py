# -*- coding: utf-8 -*-
"""rag_main.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zamVOEPU1-xbdKdI8OfbP49n8P3_p0S3
"""

import sys
import random # Restored: Used for dynamic port assignment in non-DEV environments
import threading
import time
import requests
import os
from contextlib import asynccontextmanager
from typing import TypedDict, Dict, Any
from pydantic import BaseModel
from geopy.geocoders import Nominatim
from fastapi import FastAPI
import uvicorn
import logging # <-- ADD THIS IMPORT
# Configure the logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')

# Conditional Imports for Colab
try:
    from google.colab import drive
    import nest_asyncio
    from pyngrok import ngrok
    # Removed COLAB_ENV variable calculation
except ImportError:
    pass # Colab is not available

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFacePipeline
from langgraph.graph import StateGraph, END
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# -----------------------
# CONFIGURATION BLOCK
# -----------------------
# Core application objects - MUST be initialized to None (to be set in lifespan)
hf_llm = None
VECTOR_STORE = None
retriever = None

# ====== CONFIGURATION TOGGLE (Set to False for Colab/Manual Execution and True for Docker) ======
DEV_MODE = True
# ===========================================================================

MODEL_NAME = 'google/flan-t5-large'
EMBEDDING_MODEL = 'all-MiniLM-L6-v2'

# Environment-specific variables
if DEV_MODE:
    # Docker/Local Configuration
    API_PORT = 8080
    KNOWLEDGE_FILE_PATH = "/workspace/knowledge_base.txt"
    HOST_BIND_ADDRESS = "0.0.0.0"
    print(f"**MODE:** Docker/Local (DEV_MODE = True)")
else:
    # Colab/Ngrok Configuration
    NGROK_AUTH_TOKEN = "xxxxxxxxxxxxxxxxxxxxxxxxxxxx" #update auth_code from ngrok
    # Random port assignment restored
    API_PORT = random.randint(2000,9000)
    KNOWLEDGE_FILE_PATH = "/content/drive/MyDrive/Agentic AI/knowledge_base.txt"
    HOST_BIND_ADDRESS = "127.0.0.1"
    print(f"**MODE:** Colab/Ngrok (DEV_MODE = False)")
    try:
        if 'drive' in sys.modules:
            drive.mount('/content/drive')
            print('\nWaiting for file access....')
    except Exception as e:
        print(f"Drive not mounted... {e}")
    time.sleep(5)
    # Apply for Colab uvicorn threading fix (only checking if module exists)
    if 'nest_asyncio' in sys.modules:
        nest_asyncio.apply()

print(f"**INTERNAL FASTAPI PORT:** {API_PORT}")

# --- FALLBACK KNOWLEDGE DEFINITION ---
FALLBACK_KNOWLEDGE_TEXT = """
URA Bank Loan Product Information

### Core Consumer Loans
* **Unsecured Personal Loan:**
    * Fixed Interest Rate: 6.5%
    * Maximum Tenure: 5 years
    * Maximum Loan Amount: $35,000
    * Prepayment Penalty: 1% of the remaining principal balance after the first year.

* **Home Purchase Loan:**
    * Fixed Interest Rate: 4.8% (for the first 3 years, then variable)
    * Maximum Tenure: 30 years
    * Maximum Loan Amount: $800,000
    * Prepayment Penalty: None.

### Operating Hours and Accounts
* **Main Bank Operating Hours:** Monday to Friday, **9:00 AM to 4:30 PM**. Saturday, **9:00 AM to 12:00 PM**.
* **IRA Accounts:** Individual Retirement Accounts (IRA) are only available to customers with a minimum balance of **$1,500** in a URA Bank checking or savings account.

### Specialized Secured Loans
* **Car Loan (New Vehicles Only):**
    * Maximum Amount: $50,000
    * Interest Rate: 5.5% fixed.
    * Prepayment Penalty: None.
"""
# --- END OF FALLBACK KNOWLEDGE DEFINITION ---

# -----------------------
# LLM & VECTOR STORE INITIALIZATION FUNCTIONS
# -----------------------
def read_knowledge_text(path: str):
    try:
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"Knowledge file not found at path: {path}. Using FallBack content")
        return FALLBACK_KNOWLEDGE_TEXT

def build_vector_store(text: str):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 380, chunk_overlap = 50)
    documents = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]
    embed_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL, model_kwargs={"device": "cpu"})
    vector_store = FAISS.from_documents(documents, embed_model)
    print(f"\n      --> Documents Indexed: {len(documents)}")
    return vector_store

def initialize_llm():
    model_kwargs = {}
    pipeline_kwargs = {}

    if not DEV_MODE:
        # Colab (GPU assumed)
        pipeline_kwargs['device'] = 0
    else:
        # Docker/CPU configuration
        model_kwargs['device_map'] = "cpu"
        model_kwargs['low_cpu_mem_usage'] = True

    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, **model_kwargs)
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

    pipe = pipeline(
      task='text2text-generation',
      model = model,
      tokenizer = tokenizer,
      max_new_tokens = 512,
      do_sample = True,
      temperature = 0.3,
      repetition_penalty = 1.5,
      model_kwargs = {'dtype':model.dtype,'eos_token_id':tokenizer.eos_token_id},
      **pipeline_kwargs
      )

    llm = HuggingFacePipeline(
      pipeline = pipe,
      model_kwargs = {'stop':['###']},
    )
    return llm

# -----------------------
# FASTAPI LIFESPAN MANAGER (Guarantees pre-loading before API is live)
# -----------------------
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Handles startup tasks: LLM loading, Vector Store building.
    The server will not be marked 'ready' until this context manager yields.
    """
    global hf_llm, VECTOR_STORE, retriever
    logging.info("\n[STARTUP] Starting RAG component initialization...")

    # --- Phase 1: Load LLM (The Slow Part) ---
    logging.info(f"\n[PHASE 1] Loading augmentation model: {MODEL_NAME}...")
    try:
        # This is the crucial blocking call
        hf_llm = initialize_llm()
        logging.info(f"LLM pipeline is ready.")
    except Exception as e:
        logging.error(f"Error loading model {MODEL_NAME}.... {e}")
        # In a real environment, you might re-raise, but for Colab we often exit
        sys.exit(1)

    # --- Phase 2: Build Vector Store ---
    logging.info(f"\n[PHASE 2] Building Vector Store...")
    knowledge_text = read_knowledge_text(KNOWLEDGE_FILE_PATH)
    VECTOR_STORE = build_vector_store(knowledge_text)
    logging.info("Vector Store is ready.")

    retriever = VECTOR_STORE.as_retriever(
        search_type='mmr',
        search_kwargs={'k': 2, 'fetch_k':8}
    )

    # --- PHASE 3: Startup Complete ---
    logging.info("\n[STARTUP COMPLETE] Application is now ready to receive requests.")
    yield

    """
    Application Lifetime (During yield): Crucially, the FastAPI application becomes live and starts
    accepting HTTP requests only after the yield statement is hit. The yield acts as a crucial barrier:
    no API calls can be processed until the core RAG components (hf_llm and retriever) are fully loaded
    and ready. This guarantees stability.
    """

    # --- Shutdown Tasks (optional) ---
    print("Application shutting down...")


# -----------------------
# FASTAPI APP Initialization (using the lifespan manager)
# -----------------------
app = FastAPI(title="Bank RAG AI Agent", lifespan=lifespan)

# -----------------------
# FASTAPI Endpoints
# -----------------------

@app.get("/")
def read_root():
    return {
        "status": "URA Bank AI Agent",
        "title": app.title,
        "api_docs": "Access Swagger UI at /docs for full endpoint details.",
        "mode": "Docker/Local" if DEV_MODE else "Colab/Ngrok"
    }

class QueryRequest(BaseModel):
    query: str

@app.post("/query_index", response_model=Dict[str,Any])
async def query_index(request: QueryRequest):
    query = request.query

    if hf_llm is None or retriever is None:
        return {"error": "RAG service not fully initialized. Please wait for startup logs to complete."}

    try:
        docs = retriever.invoke(query)
    except Exception as e:
        print(f"Second attempt to get relevant information from Vector Store, due to Exception {e}")
        docs = retriever.invoke(query)

    context = "\n----\n".join([d.page_content for d in docs])
    print(f"Debug: Retrieved Context for {query[:20]}....': {context[:100]}....")
    final_answer = augment_with_llm(context, query)
    return {"query":query, "index_response":final_answer, 'llm_model': MODEL_NAME}


def augment_with_llm(context: str, query: str):
    prompt = f"""
You are a professional URA Bank assistant. Answer the Question **STRICTLY AND ONLY** using the Context below.
Your final answer must be **complete**, address **ALL parts** of the user's question, and use a structured format (headings or bullet points) for multi-part questions.

**INSTRUCTIONS:**
1. Use the Context to answer the Question fully.
2. Highlight key data points with **bold**.
3. Structure the output clearly using **bold headings** for each topic requested in the query. For comparisons, **ENSURE BOTH ITEMS ARE PRESENT AND SEPARATELY DESCRIBED**.
4. **DO NOT MENTION OR INCLUDE ANY TOPIC OR FACT NOT EXPLICITLY REQUESTED IN THE QUESTION.** For example, if the question asks about 'IRA Accounts,' do not mention 'Service Fees,' even if they appear in the same context block.
5. If not available, say: "I apologize, but the specific information you requested is not available in my knowledge base. Please contact a URA Bank specialist for further assistance."
6. Do not include any information that is not directly requested in the Question.
7. End with '###'.

>>> CONTEXT START <<<
{context}
>>> CONTEXT END <<<

Question: {query}
--------
Answer:
"""

    try:
        result = hf_llm.invoke(prompt).strip()
    except Exception as e:
        result = f"LLM Generation Failed: {e}."
    if result.endswith('###'):
        result = result[:-3].strip()
    return result

@app.get('/get_branch_info')
def get_branch_info(zip_code: str = '500034'):
    geolocator = Nominatim(user_agent="ura_bank_locator")
    search_query = f"HDFC Bank {zip_code}"
    try:
        location = geolocator.geocode(search_query)
        if location:
            address = f"URA Bank Branch (Nearest to {zip_code}): {location.address}"
        else:
            address = f"URA Bank Branch not found for zip code {zip_code}."
    except Exception as e:
        address = f"Geocoding failed: {e}"
    return {"branch_address": address}

# -----------------------
# LANGGRAPH AGENT (GLOBAL DEFINITION)
# -----------------------
class AgentState(TypedDict):
    query: str
    rag_result: str


def tool_node(state: AgentState):
    # This always uses 127.0.0.1 as the call is internal to the host (Colab or Docker container)
    url = f"http://127.0.0.1:{API_PORT}/query_index"
    print(f"\nLangGraph: Calling Rag tool for query: {state['query'][:50]}....")
    # Timeout is 300 seconds for the slow inference
    response = requests.post(url, json={'query':state['query']},timeout=300)
    if response.status_code == 200:
        result = response.json().get('index_response','Error in RAG response')
    else:
        result = f"RAG tool failed with status {response.status_code}. Response: {response.text}"
    return AgentState(query=state['query'], rag_result=result)

def synthesis_node(state: AgentState):
    return AgentState(
        query = state['query'],
        rag_result =(
        f"**FINAL REPORT from LangGraph**\n"
        f"**Query:** {state['query']}\n"
        f"**Answer (via {MODEL_NAME} Augmentation):** {state['rag_result']}"
        )
    )

graph_builder = StateGraph(AgentState)
graph_builder.add_node('rag_tool',tool_node)
graph_builder.add_node('final_summary', synthesis_node)
graph_builder.set_entry_point('rag_tool')
graph_builder.add_edge('rag_tool', 'final_summary')
graph_builder.add_edge('final_summary',END)
graph = graph_builder.compile()


# -----------------------
# SERVER + NGROK/DOCKER HANDLING
# -----------------------
def run_server():
    # Binds to 0.0.0.0 for Docker, 127.0.0.1 for Colab
    # The 'lifespan' manager will block execution here until the model is loaded.
    uvicorn.run(app, host=HOST_BIND_ADDRESS, port=API_PORT, log_level='error')

def start_external_service(port=API_PORT):
    if DEV_MODE:
        # Docker/Local Placeholder
        print("\n"+ "=="*80)
        print(f'FastAPI is running inside Docker container.')
        print(f'Access via HOST BROWSER at: http://localhost:{port}')
        print(f'Swagger UI              : http://localhost:{port}/docs')
        print("\n"+ "=="*80)
        return True # Placeholder object
    else:
        # Ngrok for Colab
        try:
            if 'ngrok' not in sys.modules:
                print("NGROK module not loaded. Please ensure `!pip install pyngrok` was run.")
                return None

            ngrok.set_auth_token(NGROK_AUTH_TOKEN)
            tunnel = ngrok.connect(port)
            url = tunnel.public_url
            print("\n"+ "=="*80)
            print(f'FastAPI is live on PUBLIC ULR: {url}')
            print(f'Swagger UI                      : {url}/docs')
            print("\n"+ "=="*80)
            return tunnel
        except Exception as e:
            print(f'NGROK set up failed. {e}')
            return None

if __name__ == '__main__':

    # 1. Start Server Thread
    # The server thread will block internally due to the 'lifespan' manager until setup is complete.
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    print(f'\nWaiting 15s for uvicorn server to stabilize.... (This wait is often not strictly needed due to lifespan, but kept for legacy stability)')
    time.sleep(15) # Wait time kept for the procedural Colab structure.

    if not DEV_MODE: # COLAB/TEST MODE: Executes all tests and blocks

        tunnel_object = start_external_service(API_PORT)

        # TEST QUERIES
        test_queries = [
            "What is the fixed interest rate and the maximum tenure for the Unsecured Personal Loan compared to the Home Purchase Loan?",
            "What are the main operating hours for the bank and the requirement for IRA accounts?",
            "What is the maximum loan amount and interest rate for a Car Loan, and is there a prepayment penalty?",
        ]

        # EXECUTION: LangGraph tests and Geopy test
        for q in test_queries:
            print(f"\n-- Query: {q[:50]}...")
            final_state = graph.invoke({"query":q},config={'recursion_limit':10},timeout=300)

            print("\n" + "=" * 80)
            print(final_state['rag_result'])
            print("=" * 80 + "\n")

        print("\n--- Geopy Tool Test ---")
        try:
            # We use the internal loopback address for testing
            res = requests.get(f"http://127.0.0.1:{API_PORT}/get_branch_info?zip_code=500034")
            print(f"Branch address: {res.json().get('branch_address')}")
        except Exception as e:
            print(f"Geopy failed: {e}")

        # Blocking Loop for Colab interaction
        try:
            print("\nServer running in Colab. Press Ctrl+C to stop Uvicorn and close Ngrok tunnel.")
            while True:
                time.sleep(1)

        except KeyboardInterrupt:
            if tunnel_object and not DEV_MODE and 'ngrok' in sys.modules:
                ngrok.kill()
            print("\nShutdown complete.")

    else: # DOCKER/PROD MODE (DEV_MODE = True): Prints message and exits cleanly
        start_external_service(API_PORT) # Triggers the Docker placeholder message.
        print("\nServer running inside Docker. Uvicorn CMD is the main process.")
        pass # Script finishes cleanly.