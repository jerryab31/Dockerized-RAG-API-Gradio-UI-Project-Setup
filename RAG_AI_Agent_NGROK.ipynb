{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMei1mbK+yPUIqUY6Lu1XQf"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YKDlcb37t9RN",
        "outputId": "5d9c5539-1aba-476a-c951-68ecf6253600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.6/463.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "gradio 5.50.0 requires pydantic<=2.12.3,>=2.0, but you have pydantic 2.12.5 which is incompatible.\n",
            "google-adk 1.19.0 requires fastapi<0.119.0,>=0.115.0, but you have fastapi 0.122.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mPip installation completed !\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "fastapi uvicorn pyngrok nest-asyncio geopy \\\n",
        "langchain-core langchain-community langchain-text-splitters langchain-huggingface\\\n",
        "langgraph \\\n",
        "transformers \\\n",
        "accelerate \\\n",
        "sentence-transformers \\\n",
        "faiss-cpu \\\n",
        "pydantic \\\n",
        "--upgrade\n",
        "print('Pip installation completed !')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "# 1️⃣ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2️⃣ Path to config JSON\n",
        "CONFIG_FILE_PATH = \"/content/drive/MyDrive/Agentic AI/config.json\"\n",
        "\n",
        "# 3️⃣ Load NGROK_AUTH_TOKEN from JSON\n",
        "try:\n",
        "    with open(CONFIG_FILE_PATH, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    NGROK_AUTH_TOKEN = config.get(\"NGROK_AUTH_TOKEN\")\n",
        "    if not NGROK_AUTH_TOKEN:\n",
        "        raise ValueError(\"NGROK_AUTH_TOKEN missing in JSON\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load NGROK_AUTH_TOKEN: {e}\")\n",
        "\n",
        "\n",
        "# Cell 2: Ngrok V3 Update and Authtoken Configuration (Crucial Fixes)\n",
        "\n",
        "# 1. Update ngrok binary to V3 (Fixes ERR_NGROK_121)\n",
        "!sudo rm -f /usr/local/bin/ngrok\n",
        "!wget -q -c -nc https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -qq -n ngrok-v3-stable-linux-amd64.zip\n",
        "!mv ngrok /usr/local/bin/ngrok\n",
        "\n",
        "# 2. Cleanup conflicting files (Fixes configuration conflicts)\n",
        "!sudo rm -f /root/.ngrok2/ngrok.yml\n",
        "\n",
        "# 3. Force your authtoken to be saved (Fixes ERR_NGROK_4018)\n",
        "# Use the correct V3 command: 'ngrok authtoken <TOKEN>'\n",
        "!/usr/local/bin/ngrok authtoken {NGROK_AUTH_TOKEN}\n",
        "\n",
        "print(\"Ngrok V3 configuration complete and authtoken saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBYbBIHnlzsH",
        "outputId": "ab6425a8-4e9f-481c-9061-51ed060f2061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Ngrok V3 configuration complete and authtoken saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Libraries ======\n",
        "import sys\n",
        "import random\n",
        "import threading\n",
        "import nest_asyncio\n",
        "import time\n",
        "import requests\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from typing import TypedDict, Dict, Any\n",
        "# Dict is key and value scores: Dict[str, int] = {\"math\": 90, \"science\": 85}\n",
        "# TypedDict --> Used when you want to define the structure of a dictionary\n",
        "from pydantic import BaseModel\n",
        "\"\"\"\n",
        "from typing import TypedDict\n",
        "class Person(TypedDict):\n",
        "    name: str\n",
        "    age: int\n",
        "p1: Person = {\"name\": \"Alice\", \"age\": 25}   # OK\n",
        "p2: Person = {\"name\": \"Bob\"}                # Missing 'age'\n",
        "\n",
        "Any --> Means anything goes — use when type is unknown or can vary.\n",
        "Example:\n",
        "from typing import Any, Dict\n",
        "\n",
        "data: Dict[str, Any] = {\n",
        "    \"name\": \"Charlie\",\n",
        "    \"age\": 30,\n",
        "    \"hobbies\": [\"reading\", \"swimming\"]\n",
        "}\n",
        "Here, values can be of any type — string, int, list, etc.\n",
        "-------------------------------------------------------------\n",
        "What is BaseModel?\n",
        "BaseModel is a class from the pydantic library.\n",
        "It lets you define data models with type validation, conversion, and error checking — all automatically.\n",
        "\n",
        "BaseModel is a smart data class that:\n",
        "checks data types,\n",
        "converts types if needed,\n",
        "and raises clear errors when data is invalid.\n",
        "\n",
        "from pydantic import BaseModel\n",
        "class User(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "\n",
        "# Example data\n",
        "user1 = User(name=\"Alice\", age=25)       # Works\n",
        "user2 = User(name=\"Bob\", age=\"25\")       # Works — auto-converted to int\n",
        "user3 = User(name=\"Eve\")                 # Raises error (missing 'age')\n",
        "\n",
        "print(user2.age)  # 25 (converted to int)\n",
        "------------------------------------------------------\n",
        "BaseModel is more powerful than TypedDict, Dict, and Any.\n",
        "But let’s see why you might not always use it, even though it’s great.\n",
        "\n",
        "Although BaseModel is powerful, there are trade-offs:\n",
        "\n",
        "Performance\n",
        "BaseModel runs validation logic every time an object is created.\n",
        "For large data or loops with millions of objects → it’s slower.\n",
        "TypedDict or plain Dict are much faster because they do nothing at runtime.\n",
        "\n",
        "costs CPU & memory because validation runs every time you create an object.\n",
        "\"\"\"\n",
        "\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ],
      "metadata": {
        "id": "5NfahEzLm62M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "API stands for Application Programming Interface.\n",
        "It’s a way for two programs to talk to each other — like a messenger between systems.\n",
        "\n",
        "Think of a restaurant:\n",
        "You (the user) → ask for food (a request)\n",
        "Waiter (the API) → takes your request to the kitchen\n",
        "Kitchen (the backend system) → makes the food (processes data)\n",
        "Waiter brings it back to you (response)\n",
        "\n",
        "FastAPI is a Python framework for building APIs\n",
        "    — specifically web APIs that send and receive data over the internet.\n",
        "It’s called FastAPI because it’s both:\n",
        "Fast to run (very high performance)\n",
        "Fast to build (automatic validation, docs, and type support)\n",
        "\n",
        "FastAPI is built on top of Starlette (for web) and Pydantic (for data validation).\n",
        "Purpose of FastAPI\n",
        "To make it fast and easy to build APIs that:\n",
        "  handle HTTP requests (like GET, POST, PUT, DELETE),\n",
        "  automatically validate input data,\n",
        "  return responses in JSON,\n",
        "  generate automatic API docs (Swagger UI),\n",
        "  run super fast ~33,000 Requests/sec (approx)\n",
        "      -- (close to Node.js ~35,000 Requests/sec (approx) and Go ~40,000 Requests/sec (approx)).\n",
        "      -- Flask 3,000 – 4,000 requests/sec (approx.)\n",
        "Tool\tWhat it does\tPurpose\n",
        "FastAPI\tA Python web framework\tBuilds and runs your API (logic, routes, validation, etc.)\n",
        "Swagger UI (built into FastAPI)\n",
        "  --> A web interface\tAutomatically shows your API documentation and lets you test endpoints\n",
        "ngrok\tA tunneling tool\tExposes your local server (like FastAPI running on localhost) to the public internet\n",
        "\n",
        "Under the hood, FastAPI uses Uvicorn (an ASGI server) to actually serve those endpoints over the network.\n",
        " --> ASGI (Asynchronous Server Gateway Interface) is a specification that defines how Python web frameworks (like FastAPI) talk to web servers (like Uvicorn or Hypercorn).\n",
        "app = FastAPI(title=\"Banking RAG Agent\")\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=API_PORT) --> line starts the web server,\n",
        "- is a local server using my port, and ngrok makes it public\n",
        "Listens for HTTP requests (e.g., /query_index, /get_branch_info)\n",
        "Routes them to your Python functions (@app.post, @app.get)\n",
        "Returns JSON responses\n",
        "In short: Uvicorn = actual HTTP server, FastAPI = framework to define routes and handle logic\n",
        "uvicorn runs your local FastAPI server on 0.0.0.0:{API_PORT}.\n",
        "ngrok.connect(port) exposes that to the public internet via a temporary HTTPS URL.\n",
        "That’s why you can test it remotely — ngrok tunnels your local port.\n",
        "So yes:\n",
        "Your local FastAPI + ngrok = publicly exposed agent endpoints.\n",
        "\n",
        "ngrok URL is just the public entry point.\n",
        "The actual work—running the Uvicorn server, accessing FAISS, and\n",
        "using the LLM—is happening entirely locally on your computer.\n",
        "The tunnel is just the invisible, secure connection facilitating the communication.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4J9iaZXowdrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Every time a model generates the next word, it does NOT think in sentences.\n",
        "It predicts a list of possible next tokens with probabilities.\n",
        "\n",
        "Example:\n",
        "Model wants to pick next word after: “The cat sat on the”\n",
        "\n",
        "It produces something like:\n",
        "\n",
        "Token\tProbability\n",
        "“mat”\t0.60\n",
        "“floor”\t0.20\n",
        "“chair”\t0.05\n",
        "“bed”\t0.03\n",
        "\n",
        "\n",
        "do_sample = False (NO RANDOMNESS)\n",
        "Model always picks the top token (highest probability):\n",
        "→ Always picks “mat”\n",
        "\n",
        "do_sample = True (RANDOMNESS ALLOWED)\n",
        "Now the model samples (randomly chooses) from the probability distribution.\n",
        "Meaning:\n",
        "It might pick “mat” (60% chance)\n",
        "Or “floor” (20% chance)\n",
        "Or “chair” (5% chance)\n",
        "------------------------------------------------------------\n",
        "Temperature ONLY works when do_sample=True.\n",
        "Low temperature (0.1, 0.3)\n",
        "→ Slight randomness\n",
        "→ Mostly deterministic\n",
        "→ More factual, stable answers\n",
        "\n",
        "High temperature (0.8, 1.2)\n",
        "→ More randomness\n",
        "→ More creative, varied answers\n",
        "\n",
        "\n",
        "THE BEST ANALOGY\n",
        "do_sample = the ON/OFF switch\n",
        "\"Should I allow randomness at all?\"\n",
        "\n",
        "temperature = the intensity knob\n",
        "\"If randomness is ON, how strong should it be?\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nNsS_nTT1A6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# CONFIG\n",
        "# -----------------------\n",
        "NGROK_AUTH_TOKEN = \"\" # <- Enter NGROK TOKEN\n",
        "HF_TOKEN = \"\" # <- Enter HUGGING FACE TOKEN\n",
        "API_PORT = random.randint(2000,9000)\n",
        "print(f\"**INTERNAL FASTAPI PORT:** {API_PORT}\")\n",
        "MODEL_NAME = 'google/flan-t5-large'\n",
        "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
        "\n",
        "try:\n",
        "  drive.mount('/content/drive')\n",
        "  print('\\nWaiting for file access....')\n",
        "except Exception as e:\n",
        "  print(f\"Drive not mounted... {e}\")\n",
        "\n",
        "time.sleep(5)\n",
        "KNOWLEDGE_FILE_PATH = \"/content/drive/MyDrive/Agentic AI/knowledge_base.txt\"\n",
        "CONFIG_FILE_PATH = \"/content/drive/MyDrive/Agentic AI/config.json\"\n",
        "\n",
        "# -----------------------\n",
        "# LOAD TOKENS FROM JSON\n",
        "# -----------------------\n",
        "try:\n",
        "    with open(CONFIG_FILE_PATH, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    NGROK_AUTH_TOKEN = config.get(\"NGROK_AUTH_TOKEN\")\n",
        "    HF_TOKEN = config.get(\"HF_TOKEN\")\n",
        "    print(\"Config loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Config file not found at {CONFIG_FILE_PATH}\")\n",
        "    NGROK_AUTH_TOKEN = None\n",
        "    HF_TOKEN = None\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error decoding JSON in {CONFIG_FILE_PATH}\")\n",
        "    NGROK_AUTH_TOKEN = None\n",
        "    HF_TOKEN = None\n",
        "\n",
        "# --- NEW FALLBACK KNOWLEDGE DEFINITION ---\n",
        "FALLBACK_KNOWLEDGE_TEXT = \"\"\"\n",
        "URA Bank Loan Product Information\n",
        "\n",
        "### Core Consumer Loans\n",
        "* **Unsecured Personal Loan:**\n",
        "    * Fixed Interest Rate: 6.5%\n",
        "    * Maximum Tenure: 5 years\n",
        "    * Maximum Loan Amount: $35,000\n",
        "    * Prepayment Penalty: 1% of the remaining principal balance after the first year.\n",
        "\n",
        "* **Home Purchase Loan:**\n",
        "    * Fixed Interest Rate: 4.8% (for the first 3 years, then variable)\n",
        "    * Maximum Tenure: 30 years\n",
        "    * Maximum Loan Amount: $800,000\n",
        "    * Prepayment Penalty: None.\n",
        "\n",
        "### Operating Hours and Accounts\n",
        "* **Main Bank Operating Hours:** Monday to Friday, **9:00 AM to 4:30 PM**. Saturday, **9:00 AM to 12:00 PM**.\n",
        "* **IRA Accounts:** Individual Retirement Accounts (IRA) are only available to customers with a minimum balance of **$1,500** in a URA Bank checking or savings account.\n",
        "\n",
        "### Specialized Secured Loans\n",
        "* **Car Loan (New Vehicles Only):**\n",
        "    * Maximum Amount: $50,000\n",
        "    * Interest Rate: 5.5% fixed.\n",
        "    * Prepayment Penalty: None.\n",
        "\"\"\"\n",
        "# --- END OF FALLBACK KNOWLEDGE DEFINITION ---\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# allows you to run nested asynchronous (start a task and move on without waiting for it to finish.) code (like FastAPI or LangGraph)\n",
        "\n",
        "# -----------------------\n",
        "# LOAD LLM\n",
        "# -----------------------\n",
        "print(f\"\\nLoading augmentation model: {MODEL_NAME}\")\n",
        "# accelerate is a library from Hugging Face that manages device placement (GPU or CPU) for models.\n",
        "try:\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
        "\n",
        "  # Pipeline can load automatically, but explicitly loading is for more control.\n",
        "  pipe = pipeline(\n",
        "      task='text2text-generation',\n",
        "      model = model,\n",
        "      tokenizer = tokenizer,\n",
        "      max_new_tokens = 512,  # Maximum number of tokens the model will generate in the output\n",
        "      do_sample = True, # Whether the model should pick words randomly, True = adds variety to output, False = deterministic output.\n",
        "      temperature = 0.3, # works only when do_sample is True, Controls the randomness when sampling (lower = more deterministic, higher = more random)\n",
        "      repetition_penalty = 1.5, # Penalizes repeating the same tokens to reduce repetition in the generated text\n",
        "      model_kwargs = {'dtype':model.dtype,'eos_token_id':tokenizer.eos_token_id}\n",
        "  ) # Additional model-specific arguments:\n",
        "    # 'dtype': defines the data type of the model (e.g., float32)\n",
        "    # 'eos_token_id': tells the model when to stop generating text\n",
        "\n",
        "  # Take this HuggingFace pipeline and convert it into a LangChain-compatible LLM.\n",
        "  hf_llm = HuggingFacePipeline(\n",
        "      pipeline = pipe,\n",
        "      model_kwargs = {'stop':['###']}, #Stop generating text when you see the string ###.\n",
        "  )\n",
        "  print(\"LLM pipeline is ready.\")\n",
        "except Exception as e:\n",
        "  print(f\"Error loading model {MODEL_NAME}.... {e}\")\n",
        "  raise e\n",
        "\n",
        "# -----------------------\n",
        "# INGEST + VECTOR STORE\n",
        "# -----------------------\n",
        "def read_knowledge_text(path: str):\n",
        "  try:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "      return f.read()\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Knowledge file not found at path: {path}. Using FallBack content\")\n",
        "    return FALLBACK_KNOWLEDGE_TEXT\n",
        "\n",
        "# INDEXING (one time at startup in real world) , pre- RAG stage, but we have kept this for e2e understand\n",
        "def build_vector_store(text: str):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 380, chunk_overlap = 50)\n",
        "  documents = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]\n",
        "  embed_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "  vector_store = FAISS.from_documents(documents, embed_model)\n",
        "  \"\"\"\n",
        "  Each document is turned into an embedding, FAISS stores them in a fast vector index\n",
        "  This allows similarity search, This is your vector database.\n",
        "  \"\"\"\n",
        "  print(f\"\\n  --> Documents Indexed: {len(documents)}\")\n",
        "  return vector_store\n",
        "\n",
        "knowledge_text = read_knowledge_text(KNOWLEDGE_FILE_PATH)\n",
        "VECTOR_STORE = build_vector_store(knowledge_text)\n",
        "print(\"\\nVector Store is ready.\")\n",
        "\n",
        "retriever = VECTOR_STORE.as_retriever(\n",
        "    search_type='mmr',\n",
        "    search_kwargs={'k': 2, 'fetch_k':8})\n",
        "\n",
        "# -----------------------\n",
        "# RETRIEVER\n",
        "# -----------------------\n",
        "\"\"\"\n",
        "VECTOR_STORE.as_retriever:\n",
        "- We are creating a search object (retriever) on top of the vector store (FAISS vector database).\n",
        "MMR Retriever:\n",
        "The retriever is configured to use the MMR algorithm — Maximum Marginal Relevance.\n",
        "MMR looks at two things at the same time:\n",
        "Relevance → how closely a document matches the query\n",
        "Diversity → making sure the selected documents are not identical to each other\n",
        "\n",
        "How the process works:\n",
        "First, fetch_k = 8 →\n",
        "FAISS performs similarity search (cosine distance/EUCLIDEAN) and retrieves the top 8 similar documents.\n",
        "Out of these 8, MMR selects the first document purely by highest relevance.\n",
        "\n",
        "Then MMR calculates marginal relevance for the remaining 7 documents →\n",
        "This step ensures the next selected document is not only relevant but also different from the first one.\n",
        "Finally, it chooses k = 2 →\n",
        "So you get 2 documents that are BOTH relevant and diverse, not duplicates of each other.\n",
        "\"\"\"\n",
        "\n",
        "# -----------------------\n",
        "# AUGMENT WITH LLM (Prompt)\n",
        "# -----------------------\n",
        "\n",
        "def augment_with_llm(context: str, query: str):\n",
        "  prompt = f\"\"\"\n",
        "You are a professional URA Bank assistant. Answer the Question **STRICTLY AND ONLY** using the Context below.\n",
        "Your final answer must be **complete**, address **ALL parts** of the user's question, and use a structured format (headings or bullet points) for multi-part questions.\n",
        "\n",
        "**INSTRUCTIONS:**\n",
        "1. Use the Context to answer the Question fully.\n",
        "2. Highlight key data points with **bold**.\n",
        "3. Structure the output clearly using **bold headings** for each topic requested in the query. For comparisons, **ENSURE BOTH ITEMS ARE PRESENT AND SEPARATELY DESCRIBED**.\n",
        "4. **DO NOT MENTION OR INCLUDE ANY TOPIC OR FACT NOT EXPLICITLY REQUESTED IN THE QUESTION.** For example, if the question asks about 'IRA Accounts,' do not mention 'Service Fees,' even if they appear in the same context block.\n",
        "5. If not available, say: \"I apologize, but the specific information you requested is not available in my knowledge base. Please contact a URA Bank specialist for further assistance.\"\n",
        "6. Do not include any information that is not directly requested in the Question.\n",
        "7. End with '###'.\n",
        "\n",
        ">>> CONTEXT START <<<\n",
        "{context}\n",
        ">>> CONTEXT END <<<\n",
        "\n",
        "Question: {query}\n",
        "--------\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "  try:\n",
        "    result = hf_llm.invoke(prompt).strip()\n",
        "  except Exception as e:\n",
        "    result = f\"LLM Generation Failed: {e}.\"\n",
        "  if result.endswith('###'):\n",
        "    result = result[:-3].strip()\n",
        "  return result\n",
        "\n",
        "# -----------------------\n",
        "# FASTAPI APP\n",
        "# -----------------------\n",
        "app = FastAPI(title=\"Bank RAG AI Agent\") #Initializes the web application.\n",
        "\n",
        "\"\"\"\n",
        "“This is my root endpoint — the very first API route.\n",
        "When someone visits the API at /, this function runs and returns a small JSON status message.\n",
        "@app.get(\"/\") → means this is a GET request at the root path.\n",
        "read_root() → the function FastAPI calls.\n",
        "\"\"\"\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "  return {\n",
        "      \"status\": \"URA Bank AI Agent\",\n",
        "      \"title\": app.title,\n",
        "      \"description\": app.description,\n",
        "      \"version\": app.version,\n",
        "      \"api_docs\": \"Access Swagger UI at docs for full endpoint details.\"\n",
        "  }\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "  query: str\n",
        "\n",
        "\"\"\"\n",
        "# @app.post(\"/query_index\", ...)\tDefines an API endpoint that receives a\n",
        "user query (request: QueryRequest) and executes the RAG process.\n",
        "# end point -->> is essentially the address where a service or resource can be found.\n",
        "In our example, /query_index is the path on your server.\n",
        "# async allows your server to handle many users (queries) at the exact same time without\n",
        "one slow query blocking everyone else. It keeps the whole application responsive and fast.\n",
        "\n",
        "request is the object (or instance of the class) that holds the data.\n",
        "QueryRequest is the class that defines what the object should look like.\n",
        "query is the attribute (the specific piece of data) defined within that blueprint.\n",
        "\n",
        "response_model=Dict[str, Any]\n",
        "dictionary (Dict) where the keys are strings (str) and the values can be anything (Any).\"\n",
        "\n",
        "The @app.post decorator (which is provided by a framework like FastAPI) is a function that has\n",
        "been specifically programmed to look for a certain set of keywords.\n",
        "\"/query_index\" is the first (positional) argument: the path.\n",
        "response_model= is a predefined keyword that the framework recognizes.\n",
        "response_model acts like a configuration setting key for the FastAPI framework.\n",
        "\"\"\"\n",
        "\n",
        "@app.post(\"/query_index\", response_model=Dict[str,Any])\n",
        "async def query_index(request: QueryRequest): #request object of class QueryRequest\n",
        "  query = request.query # query is attribute of this class\n",
        "  try:\n",
        "    print(\"First attempt to get relevent information from Vector Store\")\n",
        "    docs = retriever.invoke(query)\n",
        "  except Exception as e:\n",
        "    print(f\"Second attempt to get relevent information from Vector Store, due to Exception {e}\")\n",
        "    docs = retriever.get_relevenat_documents(query)\n",
        "  context = \"\\n----\\n\".join([d.page_content for d in docs])\n",
        "  \"\"\"\n",
        "  # Vector store contains two things: the original Document objects (with page_content) and the embedding model.\n",
        "  # The retriever is a wrapper on the vector store: it embeds the query using the same embedding model,\n",
        "  # searches FAISS using the configured search type (MMR), and returns the top k=2 Document objects.\n",
        "  # Each Document contains a text chunk (page_content).\n",
        "  # We extract d.page_content to get just the text for the LLM.\n",
        "  # Joining them with '\\n----\\n' forms a single context string for the LLM input.\n",
        "  \"\"\"\n",
        "  print(f\"Debug: Retrieved Context for {query[:20]}....': {context[:100]}....\")\n",
        "  final_answer = augment_with_llm(context, query)\n",
        "  return {\"query\":query, \"index_response\":final_answer, 'llm_model': MODEL_NAME}\n",
        "\n",
        "\n",
        "@app.get('/get_branch_info')\n",
        "def get_branch_info(zip_code: str = '500034'):\n",
        "  geolocator = Nominatim(user_agent=\"ura_bank_locator\") # Nominatim (powered by OpenStreetMap data)\n",
        "  #user agent -->standard and mandatory param, It tells the service who is using their data.\n",
        "  search_query = f\"HDFC Bank {zip_code}\" #trying to find a relate branch address.\n",
        "  try:\n",
        "    location = geolocator.geocode(search_query)\n",
        "    if location:\n",
        "      address = f\"URA Bank Branch (Nearest  to {zip_code}): {location.address}\"\n",
        "    else:\n",
        "      address = address = f\"URA Bank Branch not found for zip code {zip_code}.\"\n",
        "  except Exception as e:\n",
        "    address = f\"Geocoding failed: {e}\"\n",
        "  return {\"branch_address\": address}\n",
        "\n",
        "# -----------------------\n",
        "# LANGGRAPH AGENT\n",
        "# -----------------------\n",
        "class AgentState(TypedDict):\n",
        "  query: str\n",
        "  rag_result: str\n",
        "\n",
        "# it calls the FastAPI /query_index endpoint (the RAG engine) and gets the answer.\n",
        "\"\"\"\n",
        "tool_node is a function that acts as a LangGraph node.\n",
        "Its job: take a query from the agent state, send it to the FastAPI RAG endpoint (/query_index),\n",
        "and get the response.\n",
        "\n",
        "requests.post acts as an HTTP client. Sends a POST request to /query_index.\n",
        "\"\"\"\n",
        "\n",
        "def tool_node(state: AgentState): # state - object of class AgentState\n",
        "  url = f\"http://127.0.0.1:{API_PORT}/query_index\"\n",
        "  print(f\"\\nLangGraph: Calling Rag tool for query: {state['query'][:50]}....\") #query is the attribute\n",
        "  response = requests.post(url, json={'query':state['query']},timeout=300)\n",
        "  #The requests library is being used to act as an HTTP Client to talk to your running FastAPI server's endpoint\n",
        "  if response.status_code == 200:\n",
        "    result = response.json().get('index_response','Error in RAG response')\n",
        "  else:\n",
        "    result = f\"RAG tool failed with status {response.status_code}\"\n",
        "  return AgentState(query=state['query'], rag_result=result)\n",
        "\n",
        "def synthesis_node(state: AgentState):\n",
        "  return AgentState(\n",
        "      query = state['query'],\n",
        "      rag_result =(\n",
        "      f\"**FINAL REPORT from LangGraph**\\n\"\n",
        "      f\"**Query:** {state['query']}\\n\"\n",
        "      f\"**Answer (via {MODEL_NAME} Augmentation):** {state['rag_result']}\"\n",
        "      )\n",
        "  )\n",
        "\n",
        "# -----------------------\n",
        "# SERVER + NGROK\n",
        "# -----------------------\n",
        "def run_server():\n",
        "  uvicorn.run(app, host=\"127.0.0.1\", port=API_PORT, log_level='error')\n",
        "\n",
        "def start_ngrok(port=API_PORT):\n",
        "  try:\n",
        "    if NGROK_AUTH_TOKEN:\n",
        "      ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "      tunnel = ngrok.connect(port)\n",
        "      url = tunnel.public_url\n",
        "      print(\"\\n\"+ \"==\"*80)\n",
        "      print(f'FastAPI is live on PUBLIC ULR: {url}')\n",
        "      print(f'Swagger UI                   : {url}/docs')\n",
        "      print(\"\\n\"+ \"==\"*80)\n",
        "      return tunnel\n",
        "      \"\"\"\n",
        "      Swagger UI is the graphical interface that FastAPI uses to display its API documentation.\n",
        "      All Available Endpoints: (e.g., /query_index and /get_branch_info).\n",
        "      \"\"\"\n",
        "  except Exception as e:\n",
        "    print(f'NGROK set up failed. {e}')\n",
        "  return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "  # daemon=True\n",
        "  # If the main program finishes, all running daemon threads are immediately and abruptly terminated.\n",
        "  server_thread.start()\n",
        "  print(f'\\nWaiting 15s for uvicorn server to stabilize....')\n",
        "  time.sleep(15)\n",
        "  tunnel_object = start_ngrok(API_PORT)\n",
        "\n",
        "  graph_builder = StateGraph(AgentState)\n",
        "  graph_builder.add_node('rag_tool',tool_node) #Adds the first node (RAG tool).\n",
        "  graph_builder.add_node('final_summary', synthesis_node) #Adds the second node (LLM summary)\n",
        "  graph_builder.set_entry_point('rag_tool') #Tells the graph where to begin\n",
        "  graph_builder.add_edge('rag_tool', 'final_summary') #Defines the path from RAG to summary\n",
        "  graph_builder.add_edge('final_summary',END) #Defines the stopping point.\n",
        "  graph = graph_builder.compile() # Finalizes and optimizes the graph.\n",
        "\n",
        "  test_queries = [\n",
        "        \"What is the fixed interest rate and the maximum tenure for the Unsecured Personal Loan compared to the Home Purchase Loan?\",\n",
        "        \"What are the main operating hours for the bank and the requirement for IRA accounts?\",\n",
        "        \"What is the maximum loan amount and interest rate for a Car Loan, and is there a prepayment penalty?\",\n",
        "    ]\n",
        "\n",
        "  for q in test_queries:\n",
        "    print(f\"\\n-- Query: {q[:50]}\")\n",
        "    final_state = graph.invoke({\"query\":q},config={'recursion_limit':10},timeout=300)\n",
        "    # graph.invoke() is the method used to start and run your entire LangGraph state machine.\n",
        "    # It is the command that takes the input query and executes the entire RAG (Retrieval-Augmented\n",
        "    # Generation) workflow you defined until it reaches the end point.\n",
        "    \"\"\"\n",
        "    recursion_limit sets the maximum depth of nested node calls in LangGraph.\n",
        "    In our linear graph (rag_tool → final_summary), it doesn’t affect execution,\n",
        "    but it prevents infinite loops in more complex graphs.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(final_state['rag_result'])\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "  print(\"\\n--- Geopy Tool Test ---\")\n",
        "  try:\n",
        "    res = requests.get(f\"http://127.0.0.1:{API_PORT}/get_branch_info?zip_code=500034\")\n",
        "    print(f\"Branch address: {res.json().get('branch_address')}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Geopy failed: {e}\")\n",
        "\n",
        "\n",
        "  try:\n",
        "    print(\"\\nServer running. Press Ctrl+C to stop Uvicorn and close Ngrok tunnel.\")\n",
        "\n",
        "    while True:\n",
        "      time.sleep(1)\n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "    if tunnel_object:\n",
        "      ngrok.kill()\n",
        "    print(\"\\nShutdown complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KujVa-L8ovQ6",
        "outputId": "4909c933-4628-494c-a4b0-20001f96af20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**INTERNAL FASTAPI PORT:** 5138\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Waiting for file access....\n",
            "\n",
            "Loading augmentation model: google/flan-t5-large\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM pipeline is ready.\n",
            "\n",
            "  --> Documents Indexed: 8\n",
            "\n",
            "Vector Store is ready.\n",
            "\n",
            "Waiting 15s for uvicorn server to stabilize....\n",
            "\n",
            "================================================================================================================================================================\n",
            "FastAPI is live on PUBLIC ULR: https://autoicous-daine-homelike.ngrok-free.dev\n",
            "Swagger UI                   : https://autoicous-daine-homelike.ngrok-free.dev/docs\n",
            "\n",
            "================================================================================================================================================================\n",
            "\n",
            "-- Query: What is the fixed interest rate and the maximum te\n",
            "\n",
            "LangGraph: Calling Rag tool for query: What is the fixed interest rate and the maximum te....\n",
            "First attempt to get relevent information from Vector Store\n",
            "Debug: Retrieved Context for What is the fixed in....': --- URA Bank Loan Product Information ---\n",
            "\n",
            "### Core Consumer Loans\n",
            "The fixed interest rate for the *....\n",
            "\n",
            "================================================================================\n",
            "**FINAL REPORT from LangGraph**\n",
            "**Query:** What is the fixed interest rate and the maximum tenure for the Unsecured Personal Loan compared to the Home Purchase Loan?\n",
            "**Answer (via google/flan-t5-large Augmentation):** ### The fixed interest rate for the **Unsecured Personal Loan** is **7.5%** with a maximum tenure of **60 months** (5 years). This loan is designed for immediate personal financial needs and requires no collateral. ------ The standard **Home Purchase Loan** has a **variable interest rate** (currently **6.8% to 8.2%**) and a maximum tenure of **360 months** (30 years).\n",
            "================================================================================\n",
            "\n",
            "\n",
            "-- Query: What are the main operating hours for the bank and\n",
            "\n",
            "LangGraph: Calling Rag tool for query: What are the main operating hours for the bank and....\n",
            "First attempt to get relevent information from Vector Store\n",
            "Debug: Retrieved Context for What are the main op....': **IRA Accounts:** Individual Retirement Accounts (IRA) are only available to customers with a minimu....\n"
          ]
        }
      ]
    }
  ]
}