{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtLIGPlDjeRcsFTEC6C683"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YKDlcb37t9RN",
        "outputId": "af2af234-4e78-4cee-efe0-d4748c7fc2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.2/109.2 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.5 which is incompatible.\n",
            "langchain 0.3.27 requires langchain-text-splitters<1.0.0,>=0.3.9, but you have langchain-text-splitters 1.0.0 which is incompatible.\n",
            "gradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mPip installation completed !\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "fastapi uvicorn pyngrok nest-asyncio geopy \\\n",
        "langchain-core langchain-community langchain-text-splitters langchain-huggingface\\\n",
        "langgraph \\\n",
        "transformers \\\n",
        "accelerate \\\n",
        "sentence-transformers \\\n",
        "faiss-cpu \\\n",
        "pydantic \\\n",
        "--upgrade\n",
        "print('Pip installation completed !')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "# 1️⃣ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2️⃣ Path to config JSON\n",
        "CONFIG_FILE_PATH = \"/content/drive/MyDrive/Agentic AI/config.json\"\n",
        "\n",
        "# 3️⃣ Load NGROK_AUTH_TOKEN from JSON\n",
        "try:\n",
        "    with open(CONFIG_FILE_PATH, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    NGROK_AUTH_TOKEN = config.get(\"NGROK_AUTH_TOKEN\")\n",
        "    if not NGROK_AUTH_TOKEN:\n",
        "        raise ValueError(\"NGROK_AUTH_TOKEN missing in JSON\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"Failed to load NGROK_AUTH_TOKEN: {e}\")\n",
        "\n",
        "\n",
        "# Cell 2: Ngrok V3 Update and Authtoken Configuration (Crucial Fixes)\n",
        "\n",
        "# 1. Update ngrok binary to V3 (Fixes ERR_NGROK_121)\n",
        "!sudo rm -f /usr/local/bin/ngrok\n",
        "!wget -q -c -nc https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.zip\n",
        "!unzip -qq -n ngrok-v3-stable-linux-amd64.zip\n",
        "!mv ngrok /usr/local/bin/ngrok\n",
        "\n",
        "# 2. Cleanup conflicting files (Fixes configuration conflicts)\n",
        "!sudo rm -f /root/.ngrok2/ngrok.yml\n",
        "\n",
        "# 3. Force your authtoken to be saved (Fixes ERR_NGROK_4018)\n",
        "# Use the correct V3 command: 'ngrok authtoken <TOKEN>'\n",
        "!/usr/local/bin/ngrok authtoken {NGROK_AUTH_TOKEN}\n",
        "\n",
        "print(\"✅ Ngrok V3 configuration complete and authtoken saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBYbBIHnlzsH",
        "outputId": "9d66f0c7-2e67-406b-fa45-817d2e5a840b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "✅ Ngrok V3 configuration complete and authtoken saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== Libraries ======\n",
        "import sys\n",
        "import random\n",
        "import threading\n",
        "import nest_asyncio\n",
        "import time\n",
        "import requests\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from typing import TypedDict, Dict, Any\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline"
      ],
      "metadata": {
        "id": "5NfahEzLm62M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# CONFIG\n",
        "# -----------------------\n",
        "NGROK_AUTH_TOKEN = \"\" # <- Enter NGROK TOKEN\n",
        "HF_TOKEN = \"\" # <- Enter HUGGING FACE TOKEN\n",
        "API_PORT = random.randint(2000,9000)\n",
        "print(f\"**INTERNAL FASTAPI PORT:** {API_PORT}\")\n",
        "MODEL_NAME = 'google/flan-t5-large'\n",
        "EMBEDDING_MODEL = 'all-MiniLM-L6-v2'\n",
        "\n",
        "try:\n",
        "  drive.mount('/content/drive')\n",
        "  print('\\nWaiting for file access....')\n",
        "except Exception as e:\n",
        "  print(f\"Drive not mounted... {e}\")\n",
        "\n",
        "time.sleep(5)\n",
        "KNOWLEDGE_FILE_PATH = \"/content/drive/MyDrive/Agentic AI/knowledge_base.txt\"\n",
        "CONFIG_FILE_PATH = \"/content/drive/MyDrive/Agentic AI/config.json\"\n",
        "\n",
        "# -----------------------\n",
        "# LOAD TOKENS FROM JSON\n",
        "# -----------------------\n",
        "try:\n",
        "    with open(CONFIG_FILE_PATH, \"r\") as f:\n",
        "        config = json.load(f)\n",
        "    NGROK_AUTH_TOKEN = config.get(\"NGROK_AUTH_TOKEN\")\n",
        "    HF_TOKEN = config.get(\"HF_TOKEN\")\n",
        "    print(\"Config loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Config file not found at {CONFIG_FILE_PATH}\")\n",
        "    NGROK_AUTH_TOKEN = None\n",
        "    HF_TOKEN = None\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error decoding JSON in {CONFIG_FILE_PATH}\")\n",
        "    NGROK_AUTH_TOKEN = None\n",
        "    HF_TOKEN = None\n",
        "\n",
        "# --- NEW FALLBACK KNOWLEDGE DEFINITION ---\n",
        "FALLBACK_KNOWLEDGE_TEXT = \"\"\"\n",
        "URA Bank Loan Product Information\n",
        "\n",
        "### Core Consumer Loans\n",
        "* **Unsecured Personal Loan:**\n",
        "    * Fixed Interest Rate: 6.5%\n",
        "    * Maximum Tenure: 5 years\n",
        "    * Maximum Loan Amount: $35,000\n",
        "    * Prepayment Penalty: 1% of the remaining principal balance after the first year.\n",
        "\n",
        "* **Home Purchase Loan:**\n",
        "    * Fixed Interest Rate: 4.8% (for the first 3 years, then variable)\n",
        "    * Maximum Tenure: 30 years\n",
        "    * Maximum Loan Amount: $800,000\n",
        "    * Prepayment Penalty: None.\n",
        "\n",
        "### Operating Hours and Accounts\n",
        "* **Main Bank Operating Hours:** Monday to Friday, **9:00 AM to 4:30 PM**. Saturday, **9:00 AM to 12:00 PM**.\n",
        "* **IRA Accounts:** Individual Retirement Accounts (IRA) are only available to customers with a minimum balance of **$1,500** in a URA Bank checking or savings account.\n",
        "\n",
        "### Specialized Secured Loans\n",
        "* **Car Loan (New Vehicles Only):**\n",
        "    * Maximum Amount: $50,000\n",
        "    * Interest Rate: 5.5% fixed.\n",
        "    * Prepayment Penalty: None.\n",
        "\"\"\"\n",
        "# --- END OF FALLBACK KNOWLEDGE DEFINITION ---\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# -----------------------\n",
        "# LOAD LLM\n",
        "# -----------------------\n",
        "print(f\"\\nLoading augmentation model: {MODEL_NAME}\")\n",
        "try:\n",
        "  model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN)\n",
        "\n",
        "    pipe = pipeline(\n",
        "      task='text2text-generation',\n",
        "      model = model,\n",
        "      tokenizer = tokenizer,\n",
        "      max_new_tokens = 512,\n",
        "      do_sample = True,\n",
        "      temperature = 0.3,\n",
        "      repetition_penalty = 1.5,\n",
        "      model_kwargs = {'dtype':model.dtype,'eos_token_id':tokenizer.eos_token_id}\n",
        "  )\n",
        "\n",
        "\n",
        "  hf_llm = HuggingFacePipeline(\n",
        "      pipeline = pipe,\n",
        "      model_kwargs = {'stop':['###']},\n",
        "  )\n",
        "  print(\"LLM pipeline is ready.\")\n",
        "except Exception as e:\n",
        "  print(f\"Error loading model {MODEL_NAME}.... {e}\")\n",
        "  raise e\n",
        "\n",
        "# -----------------------\n",
        "# INGEST + VECTOR STORE\n",
        "# -----------------------\n",
        "def read_knowledge_text(path: str):\n",
        "  try:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "      return f.read()\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Knowledge file not found at path: {path}. Using FallBack content\")\n",
        "    return FALLBACK_KNOWLEDGE_TEXT\n",
        "\n",
        "def build_vector_store(text: str):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 380, chunk_overlap = 50)\n",
        "  documents = [Document(page_content=chunk) for chunk in text_splitter.split_text(text)]\n",
        "  embed_model = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
        "  vector_store = FAISS.from_documents(documents, embed_model)\n",
        "  print(f\"\\n  --> Documents Indexed: {len(documents)}\")\n",
        "  return vector_store\n",
        "\n",
        "knowledge_text = read_knowledge_text(KNOWLEDGE_FILE_PATH)\n",
        "VECTOR_STORE = build_vector_store(knowledge_text)\n",
        "print(\"\\nVector Store is ready.\")\n",
        "\n",
        "retriever = VECTOR_STORE.as_retriever(\n",
        "    search_type='mmr',\n",
        "    search_kwargs={'k': 2, 'fetch_k':8})\n",
        "\n",
        "# -----------------------\n",
        "# AUGMENT WITH LLM (Prompt)\n",
        "# -----------------------\n",
        "\n",
        "def augment_with_llm(context: str, query: str):\n",
        "  prompt = f\"\"\"\n",
        "You are a professional URA Bank assistant. Answer the Question **STRICTLY AND ONLY** using the Context below.\n",
        "Your final answer must be **complete**, address **ALL parts** of the user's question, and use a structured format (headings or bullet points) for multi-part questions.\n",
        "\n",
        "**INSTRUCTIONS:**\n",
        "1. Use the Context to answer the Question fully.\n",
        "2. Highlight key data points with **bold**.\n",
        "3. Structure the output clearly using **bold headings** for each topic requested in the query. For comparisons, **ENSURE BOTH ITEMS ARE PRESENT AND SEPARATELY DESCRIBED**.\n",
        "4. **DO NOT MENTION OR INCLUDE ANY TOPIC OR FACT NOT EXPLICITLY REQUESTED IN THE QUESTION.** For example, if the question asks about 'IRA Accounts,' do not mention 'Service Fees,' even if they appear in the same context block.\n",
        "5. If not available, say: \"I apologize, but the specific information you requested is not available in my knowledge base. Please contact a URA Bank specialist for further assistance.\"\n",
        "6. Do not include any information that is not directly requested in the Question.\n",
        "7. End with '###'.\n",
        "\n",
        ">>> CONTEXT START <<<\n",
        "{context}\n",
        ">>> CONTEXT END <<<\n",
        "\n",
        "Question: {query}\n",
        "--------\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "  try:\n",
        "    result = hf_llm.invoke(prompt).strip()\n",
        "  except Exception as e:\n",
        "    result = f\"LLM Generation Failed: {e}.\"\n",
        "  if result.endswith('###'):\n",
        "    result = result[:-3].strip()\n",
        "  return result\n",
        "\n",
        "# -----------------------\n",
        "# FASTAPI APP\n",
        "# -----------------------\n",
        "app = FastAPI(title=\"Bank RAG AI Agent\")\n",
        "\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "  return {\n",
        "      \"status\": \"URA Bank AI Agent\",\n",
        "      \"title\": app.title,\n",
        "      \"description\": app.description,\n",
        "      \"version\": app.version,\n",
        "      \"api_docs\": \"Access Swagger UI at docs for full endpoint details.\"\n",
        "  }\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "  query: str\n",
        "\n",
        "@app.post(\"/query_index\", response_model=Dict[str,Any])\n",
        "async def query_index(request: QueryRequest):\n",
        "  query = request.query\n",
        "  try:\n",
        "    print(\"First attempt to get relevent information from Vector Store\")\n",
        "    docs = retriever.invoke(query)\n",
        "  except Exception as e:\n",
        "    print(f\"Second attempt to get relevent information from Vector Store, due to Exception {e}\")\n",
        "    docs = retriever.get_relevenat_documents(query)\n",
        "  context = \"\\n----\\n\".join([d.page_content for d in docs])\n",
        "  print(f\"Debug: Retrieved Context for {query[:20]}....': {context[:100]}....\")\n",
        "  final_answer = augment_with_llm(context, query)\n",
        "  return {\"query\":query, \"index_response\":final_answer, 'llm_model': MODEL_NAME}\n",
        "\n",
        "\n",
        "@app.get('/get_branch_info')\n",
        "def get_branch_info(zip_code: str = '500034'):\n",
        "  geolocator = Nominatim(user_agent=\"ura_bank_locator\")\n",
        "  search_query = f\"HDFC Bank {zip_code}\"\n",
        "  try:\n",
        "    location = geolocator.geocode(search_query)\n",
        "    if location:\n",
        "      address = f\"URA Bank Branch (Nearest  to {zip_code}): {location.address}\"\n",
        "    else:\n",
        "      address = address = f\"URA Bank Branch not found for zip code {zip_code}.\"\n",
        "  except Exception as e:\n",
        "    address = f\"Geocoding failed: {e}\"\n",
        "  return {\"branch_address\": address}\n",
        "\n",
        "# -----------------------\n",
        "# LANGGRAPH AGENT\n",
        "# -----------------------\n",
        "class AgentState(TypedDict):\n",
        "  query: str\n",
        "  rag_result: str\n",
        "\n",
        "\n",
        "def tool_node(state: AgentState):\n",
        "  url = f\"http://127.0.0.1:{API_PORT}/query_index\"\n",
        "  print(f\"\\nLangGraph: Calling Rag tool for query: {state['query'][:50]}....\")\n",
        "  response = requests.post(url, json={'query':state['query']},timeout=300)\n",
        "  if response.status_code == 200:\n",
        "    result = response.json().get('index_response','Error in RAG response')\n",
        "  else:\n",
        "    result = f\"RAG tool failed with status {response.status_code}\"\n",
        "  return AgentState(query=state['query'], rag_result=result)\n",
        "\n",
        "def synthesis_node(state: AgentState):\n",
        "  return AgentState(\n",
        "      query = state['query'],\n",
        "      rag_result =(\n",
        "      f\"**FINAL REPORT from LangGraph**\\n\"\n",
        "      f\"**Query:** {state['query']}\\n\"\n",
        "      f\"**Answer (via {MODEL_NAME} Augmentation):** {state['rag_result']}\"\n",
        "      )\n",
        "  )\n",
        "\n",
        "# -----------------------\n",
        "# SERVER + NGROK\n",
        "# -----------------------\n",
        "def run_server():\n",
        "  uvicorn.run(app, host=\"127.0.0.1\", port=API_PORT, log_level='error')\n",
        "\n",
        "def start_ngrok(port=API_PORT):\n",
        "  try:\n",
        "    if NGROK_AUTH_TOKEN:\n",
        "      ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "      tunnel = ngrok.connect(port)\n",
        "      url = tunnel.public_url\n",
        "      print(\"\\n\"+ \"==\"*80)\n",
        "      print(f'FastAPI is live on PUBLIC ULR: {url}')\n",
        "      print(f'Swagger UI                   : {url}/docs')\n",
        "      print(\"\\n\"+ \"==\"*80)\n",
        "      return tunnel\n",
        "  except Exception as e:\n",
        "    print(f'NGROK set up failed. {e}')\n",
        "  return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  server_thread = threading.Thread(target=run_server, daemon=True)\n",
        "  server_thread.start()\n",
        "  print(f'\\nWaiting 15s for uvicorn server to stabilize....')\n",
        "  time.sleep(15)\n",
        "  tunnel_object = start_ngrok(API_PORT)\n",
        "\n",
        "  graph_builder = StateGraph(AgentState)\n",
        "  graph_builder.add_node('rag_tool',tool_node)\n",
        "  graph_builder.add_node('final_summary', synthesis_node)\n",
        "  graph_builder.set_entry_point('rag_tool')\n",
        "  graph_builder.add_edge('rag_tool', 'final_summary')\n",
        "  graph_builder.add_edge('final_summary',END)\n",
        "  graph = graph_builder.compile()\n",
        "\n",
        "  test_queries = [\n",
        "        \"What is the fixed interest rate and the maximum tenure for the Unsecured Personal Loan compared to the Home Purchase Loan?\",\n",
        "        \"What are the main operating hours for the bank and the requirement for IRA accounts?\",\n",
        "        \"What is the maximum loan amount and interest rate for a Car Loan, and is there a prepayment penalty?\",\n",
        "    ]\n",
        "\n",
        "  for q in test_queries:\n",
        "    print(f\"\\n-- Query: {q[:50]}\")\n",
        "    final_state = graph.invoke({\"query\":q},config={'recursion_limit':10},timeout=300)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(final_state['rag_result'])\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "  print(\"\\n--- Geopy Tool Test ---\")\n",
        "  try:\n",
        "    res = requests.get(f\"http://127.0.0.1:{API_PORT}/get_branch_info?zip_code=500034\")\n",
        "    print(f\"Branch address: {res.json().get('branch_address')}\")\n",
        "  except Exception as e:\n",
        "    print(f\"Geopy failed: {e}\")\n",
        "\n",
        "\n",
        "  try:\n",
        "    print(\"\\nServer running. Press Ctrl+C to stop Uvicorn and close Ngrok tunnel.\")\n",
        "\n",
        "    while True:\n",
        "      time.sleep(1)\n",
        "\n",
        "  except KeyboardInterrupt:\n",
        "    if tunnel_object:\n",
        "      ngrok.kill()\n",
        "    print(\"\\nShutdown complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KujVa-L8ovQ6",
        "outputId": "4909c933-4628-494c-a4b0-20001f96af20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**INTERNAL FASTAPI PORT:** 5138\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "Waiting for file access....\n",
            "\n",
            "Loading augmentation model: google/flan-t5-large\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM pipeline is ready.\n",
            "\n",
            "  --> Documents Indexed: 8\n",
            "\n",
            "Vector Store is ready.\n",
            "\n",
            "Waiting 15s for uvicorn server to stabilize....\n",
            "\n",
            "================================================================================================================================================================\n",
            "FastAPI is live on PUBLIC ULR: https://autoicous-daine-homelike.ngrok-free.dev\n",
            "Swagger UI                   : https://autoicous-daine-homelike.ngrok-free.dev/docs\n",
            "\n",
            "================================================================================================================================================================\n",
            "\n",
            "-- Query: What is the fixed interest rate and the maximum te\n",
            "\n",
            "LangGraph: Calling Rag tool for query: What is the fixed interest rate and the maximum te....\n",
            "First attempt to get relevent information from Vector Store\n",
            "Debug: Retrieved Context for What is the fixed in....': --- URA Bank Loan Product Information ---\n",
            "\n",
            "### Core Consumer Loans\n",
            "The fixed interest rate for the *....\n",
            "\n",
            "================================================================================\n",
            "**FINAL REPORT from LangGraph**\n",
            "**Query:** What is the fixed interest rate and the maximum tenure for the Unsecured Personal Loan compared to the Home Purchase Loan?\n",
            "**Answer (via google/flan-t5-large Augmentation):** ### The fixed interest rate for the **Unsecured Personal Loan** is **7.5%** with a maximum tenure of **60 months** (5 years). This loan is designed for immediate personal financial needs and requires no collateral. ------ The standard **Home Purchase Loan** has a **variable interest rate** (currently **6.8% to 8.2%**) and a maximum tenure of **360 months** (30 years).\n",
            "================================================================================\n",
            "\n",
            "\n",
            "-- Query: What are the main operating hours for the bank and\n",
            "\n",
            "LangGraph: Calling Rag tool for query: What are the main operating hours for the bank and....\n",
            "First attempt to get relevent information from Vector Store\n",
            "Debug: Retrieved Context for What are the main op....': **IRA Accounts:** Individual Retirement Accounts (IRA) are only available to customers with a minimu....\n"
          ]
        }
      ]
    }
  ]
}